<!DOCTYPE html>
<head>
    <meta charset="UTF-8">
    <link rel="stylesheet" href="css/styles.css">
    <link rel="icon" href="images/icon.png">
    <!-- <script src="js/main.js"></script> -->
    <title>stance</title>
</head>
<body>
    <div class="bar"></div>
    <table class="menu">
        <tbody>
          <tr>
            <td class="html"><a href="proposal.html">proposal</a></td>
            <td class="html"><a href="planning.html">planning</a></td>
            <td class="html"><a href="progress.html">progress</a></td>
            <td class="html"><a href="final_update.html">final update</a></td>
            <td class="html"><a href="https://github.com/RFinkelberg/Stance">code</a></td>
            <td class="html"><a href="docs.html">docs</a></td>
          </tr>
        </tbody>
    </table>
    <h1><a href="stance.html">stance</a></h1>
    <h4>Aadarsh Padiyath, Emilee Sisson, Jorge Betancourt, Roy Finkelberg</h4>
    <h3>Abstract</h3>
    <p>Determining the similarity between abstract scenes is a difficult problem in cognitive computing. Humans can look at two images or videos and immediately tell if they are similar, but this intuition is difficult to encode.
    <br><br>
    We present an application of 2D human pose estimation for accessing similarity between motions in the space of weightlifting. Given input video of a person performing an exercise, our system will access their form. That is, it will give a similarity score between the user's motion and that motion performed "ideally".</p>
    <h3>Teaser Figure</h3>
    <center>
        <table>
            <tbody>
                <tr>
                    <td>Input</td>
                    <td>Template</td>
                    <td>Overlay</td>
                </tr>
                <tr>
                    <td><img src="images/input.png" height="351" width="170">
                    </td>
                    <td><img src="images/muscleman.png" height="351" width="170">
                    </td>
                    <td><img src="images/overlay.png" height="351" width="170">
                    </td>
                </tr>
            </tbody>
        </table>
    </center>
    <h3>Introduction</h3>
    <p>Current pose estimation techniques are able to map a wire frame onto video of a moving person. We present an application of 2D pose estimation for assessment of proper workout form. Given input video of a person performing an exercise, our system will compare the subject's wire frame with that of a template representing "ideal" form. The system will then superimpose the template with the input and return the resulting overlay with a performance metric representing the similarity between the two.</p>
    <h3>Approach</h3>
    <p>
        <ol>
            <li>For each frame in an input video, compute a wire skeleton for the person in the frame</li>
            <li>Compare each skeleton to "template skeletons" to determine if the person is in a benchmark zone</li>
            <li>To determine when the person is in a benchmark zone, compute the peaks of the similarity scores from the previous step</li>
            <li>Sample a window of skeletons centered at these peaks</li>
            <li>Compute the average similarity over the window with the template skeleton</li>
            <li>Compute the performance score to be a weighted sum of the similarities at each benchmark zone</li>
        </ol>
        Full flow diagram and modules are on the <a href="planning.html">planning</a> page.
        <br><br>
        <center><img src="images/static_diagram.png"/></center>
        <h4>Major Design Decisions</h4>
        <p><b>Skeleton Representation - </b>The problem with representing skeletons as the coordinates as their keypoints is that they are now sensitive to the scale and orientation of the subject. Our system represents skeletons as sets of relationships between points (i.e. distance, relative angle, ...). Looking at these relationships rather than the absolute points makes our system robust to changes in the composition of the shot.</p>
    </p> 
    <h3>Experimentation</h3>
    <!--Provide details about the experimental set up (number of images/videos, number of datasets you experimented with, train/test split if you used machine learning algorithms, etc.). Describe the evaluation metrics you used to evaluate how well your approach is working. Include clear figures and tables, as well as illustrative qualitative examples if appropriate. Be sure to include obvious baselines to see if your approach is doing better than a naive approach (e.g. for classification accuracy, how well would a classifier do that made random decisions?). Also discuss any parameters of your algorithms, and tell us how you set the values of those parameters. You can also show us how the performance varies as you change those parameter values. Be sure to discuss any trends you see in your results, and explain why these trends make sense. Are the results as expected? Why?-->
    <!--Provide details about the experimental set up (number of images/videos, number of datasets you experimented with, train/test split if you used machine learning algorithms, etc.). Describe the evaluation metrics you used to evaluate how well your approach is working. Include clear figures and tables, as well as illustrative qualitative examples if appropriate. Be sure to include obvious baselines to see if your approach is doing better than a naive approach (e.g. for classification accuracy, how well would a classifier do that made random decisions?). Also discuss any parameters of your algorithms, and tell us how you set the values of those parameters. You can also show us how the performance varies as you change those parameter values. Be sure to discuss any trends you see in your results, and explain why these trends make sense. Are the results as expected? Why?-->
    <p>When evaluating the performance of our system with regard to fitting skeletons, we had to do so qualitatively. A quantitative measure of this performance would require some notion of a "perfectly fit skeleton." If we could compute such a thing, we would be using it to solve our problem.</p>
    <!--Show several visual examples of inputs/outputs of your system (success cases and failures) that help us better understand your approach.-->
    <p>We were able to use the OpenCV Software along with a COCO object detection neural network to detect the skeleton of a user in a single photo.</p>
    <center>
        <img src="images/man-skeleton.jpg" height="300px">
    </center>
    <div id="squatvideo">
        <h4>Extension to Adversarial Video</h4>
        <p>We extended this process to detect the skeletons of a user performing a squatting exercise. This video is adversarial because of the color composition of the video - the background is very low contrast with the person's clothing.</p>
        <center>
            <embed src="images/output.swf" height="300px" width="530px"/>
        </center>
    </div>
    <p>Dispite the nature of the input, the system tracked the person very well. The front leg is stable enough to use in similarity calculations.</p>
    <h4>Partial occlusion</h4>
    <p>We decided to test the neural net on multiple of our own images to see if we could achieve good results. We set up the images with a consistent subject against a non-trivial background. In real-world scenarios, occlusion is a problem. In squatting specifically, the plates and squat rack will be blocking the view of the camera.</p>
    <center>
        <table>
            <tbody>
                <tr>
                    <td><img src="images/Roy1.JPG" height="351" width="200">
                    </td>
                    <td><img src="images/Roy2.JPG" height="351" width="200">
                    </td>
                    <td><img src="images/Roy3.JPG" height="351" width="200">
                    </td>
                </tr>
            </tbody>
        </table>
    </center>
    <p>We found it performed well even with differing amounts of partial occulusion and poses.</p>
    <h4>Viewing Angles</h4>
    <p>Our system makes the assumption of a consistent viewing angle of the user's motion. One major decision we had to make was choosing that viewing angle.</p>
    <center>
        <table>
            <tbody>
                <tr>
                    <td><img src="images/Aadarsh1.JPG" height="551" width="200">
                    </td>
                    <td><img src="images/Aadarsh2.JPG" height="551" width="200">
                    </td>
                    <td><img src="images/Aadarsh3.JPG" height="551" width="200">
                    </td>
                </tr>
            </tbody>
        </table>
    </center>
    <p>Initially we thought that a profile view would be best because it removes depth - a major obstacle in pose estimation. After running these tests, we found that performance was similar across differing viewing angles.</p>
    <h4>Subject and Obstacle</h4>
    <p>One problem we foresaw with using the system in a real-world environment is that in the real world, there are often other people. We wanted to see to what extent does another person in the shot affect the quality of the output.</p>
    <center>
        <table>
            <tbody>
                <tr>
                    <td><img src="images/Depth1.JPG" height="351" width="200">
                    </td>
                    <td><img src="images/Depth2.JPG" height="351" width="200">
                    </td>
                    <td><img src="images/Depth3.JPG" height="351" width="200">
                    </td>
                </tr>
            </tbody>
        </table>
    </center>
    <p>If you have an obstacle close to the camera, keypoints will be split between the two, making for unusable data. If you have an obstacle in the background of the frame, the system might interpret it as the subject.</p>
    <h4>Scoring</h4>
    <p>For the video shown <a href="#squatvideo">here</a>, we compared the similarity of each wire skeleton to the benchmark zones of the "squat" motion and plotted it.</p>
    <center>
        <img src="images/find_benchmark_zones_wo_gauss.png"/>
    </center>
    <p>
    We instantly saw the problem of having a symmetric motion with almost symmetric benchmark zones, we would need to separate the motion into multiple sections. We decided to solve this by adding gaussians to each curve where the mean would be approximately where in the video the user should be performing that action. The plot below clearly shows the peaks of where the benchmark zones of this specific user's motion is.
    </p>
    <center>
        <img src="images/find_benchmark_zones_w_gauss.png"/>
    </center>
    <p>
        Notice that even thought the video is symmetrical, Each benchmark zone has a distinct peak and are noticiably different allowing us to identify each one.
    </p>
    <h4>Qualitative Results</h4>
    <!--This got this score for this reason-->
    <center>
        <table>
            <tbody>
                <tr>
                    <th>Video</th>
                    <th>Score</th>
                    <th>Discussion</th>
                </tr>
                <tr>
                    <td><embed src="images/squat_output_video.swf" height="300px" width="530px"/></td>
                    <td>97.9%</td>
                    <td>This was the main template we used, so it should theoretically have 100% accuracy. In reality, it has a little less because our addition of the gaussian noise makes the algorithm miss the middle benchmark zone slightly.</td>
                </tr>
                <tr>
                    <td><embed src="images/squatgood_output_video.swf" height="300px" width="530px"/></td>
                    <td>99.1%</td>
                    <td>This is an example of a user performing a "good" squat that the algorithm also agreed was good. The keypoint detector is shown to be robust to occlusion and noise.</td>
                </tr>
                <tr>
                    <td><embed src="images/squatbad_output_video.swf" height="300px" width="530px"/></td>
                    <td>87.8%</td>
                    <td>This video is an example of a user that scores well with our algorithm, but is doing the squatting motion incorrectly. They are sitting too far down, and butt juts out when ascending. However, since all of these mistakes are made inbetween benchmark zones, the system still decides to score it well. This shows the limitations of using benchmark zones to score the motion.</td>
                </tr>
                <tr>
                    <td><embed src="images/squatpoop_output_video.swf" height="300px" width="530px"/></td>
                    <td>80.1%</td>
                    <td>This video is an example of a user performing a "bad" squat and the system giving the user a bad score.</td>
                </tr>
                <tr>
                    <td><embed src="images/walk_output_video.swf" height="300px" width="530px"/></td>
                    <td>45.0</td>
                    <td>This is a degenerate case where the user is performing an entirely different motion. Since the system gave it a low score, we can consider this a success. The algorithm was able to distinguish between completely different motions.</td>
                </tr>
            </tbody>
        </table>
    </center>
    <h3>Conclusion and Future Work</h3>
    <!-- Conclusion would likely make the same points as the abstract. Discuss any future ideas you have to make your approach better.-->
    <p>
        We had two main questions after coming up with this project: How well can we distinguish between two separate motions? How well can we algorithmically grade someone's workout form?

        We were able to create a reasonable judge of workout form under "ideal" conditions. However, this required a few strong assumptions.
        <ul>
            <li><b>We only grade half the body.</b> We did this because the keypoint detector was having diffculty finding the keypoints of the hidden limbs, and would then guess where the keypoint was. This would give a lot of noise to the scored result.</li>
            <li><b>The video is a fluid motion and encompasses only the motion in question.</b> This is necessary because when we add gaussians to the motions, it doesn't take into account where the person is in the motion. So if a user were to try to grade a video of their squat where the video started with 10 seconds of them standing still and then performing the motion, it would fail to apply the gaussians correctly and separate the symmetric motion.</li>
            <li><b>We assume cosine similarity is a good similarity metric.</b> We needed to assume this because we wanted our vector comparisons to be scale invariant. This means we are sacrificing information for robustness.</li>
            <!--Add More Assumptions-->
        </ul>
        In a general sense, we were successful because we were able to distinguish between different motions. In the last video above, the algorithm gave a low enough score that it was able to significantly say that the motion was not a squat. This means we've mathmatically encoded an idea of similarity in these abstract scenes.
    </p>
    <p>Possible Extensions:
        <ul>
            <li>3D Pose Estimation for recording at any angle</li>
            <li>Automated benchmark zone finder</li>
            <li>Use <a href="http://densepose.org/">DensePose</a> to get finer detailed skeletons</li>
            <li>Instead of benchmark zones, compare the paths of certain points throughout the motion</li>
            <li>Experiment with different numbers of benchmark zones</li>
            <li>Experiment with different methods of computing skeleton similarities</li>
        </ul>
    </p>
    <h3>References</h3>
    <p><a href="https://arxiv.org/abs/1611.08050">Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields</a> - Cao et al. 2017</p>
    <p><a href="https://arxiv.org/abs/1704.07809">Hand Keypoint Detection in Single Images using Multiview Bootstrapping</a> - Simon et al. 2017</p>
    <p><a href="https://arxiv.org/abs/1602.00134">Convolutional Pose Machines</a> - Wei et al. 2016</p>
    <p><a href="http://cocodataset.org/#home">COCO Object Detector</a> - Common Objects in Context</p>
    </body>
