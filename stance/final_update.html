<!DOCTYPE html>
<head>
    <meta charset="UTF-8">
    <link rel="stylesheet" href="css/styles.css">
    <link rel="icon" href="images/icon.png">
    <!-- <script src="js/main.js"></script> -->
    <title>stance</title>
</head>
<body>
    <div class="bar"></div>
    <table class="menu">
        <tbody>
          <tr>
            <td class="html"><a href="proposal.html">proposal</a></td>
            <td class="html"><a href="planning.html">planning</a></td>
            <td class="html"><a href="progress.html">progress</a></td>
            <td class="html"><a href="final_update.html">final update</a></td>
            <td class="html"><a href="https://github.com/RFinkelberg/Stance">code</a></td>
            <td class="html"><a href="docs.html">docs</a></td>
          </tr>
        </tbody>
    </table>
    <h1><a href="stance.html">stance</a></h1>
    <h4>Aadarsh Padiyath, Emilee Sisson, Jorge Betancourt, Roy Finkelberg</h4>
    <h3>Abstract</h3>
    <p>Determining the similarity between abstract scenes is a difficult problem in cognitive computing. Humans can look at two images or videos and immediately tell if they are similar, but this intuition is difficult to encode.
    <br><br>
    We present an application of 2D human pose estimation for accessing similarity between motions in the space of weightlifting. Given input video of a person performing an exercise, our system will access their form. That is, it will give a similarity score between the user's motion and that motion performed "ideally".</p>
    <h3>Teaser Figure</h3>
    <center>
        <table>
            <tbody>
                <tr>
                    <td>Input</td>
                    <td>Template</td>
                    <td>Overlay</td>
                </tr>
                <tr>
                    <td><img src="images/input.png" height="351" width="170">
                    </td>
                    <td><img src="images/muscleman.png" height="351" width="170">
                    </td>
                    <td><img src="images/overlay.png" height="351" width="170">
                    </td>
                </tr>
            </tbody>
        </table>
    </center>
    <h3>Introduction</h3>
    <p>Current pose estimation techniques are able to map a wire frame onto video of a moving person. We present an application of 2D pose estimation for assessment of proper workout form. Given input video of a person performing an exercise, our system will compare the subject's wire frame with that of a template representing "ideal" form. The system will then superimpose the template with the input and return the resulting overlay with a performance metric representing the similarity between the two.</p>
    <h3>Approach</h3>
    <p>
        <ol>
            <li>For each frame in an input video, compute a wire skeleton for the person in the frame</li>
            <li>Compare each skeleton to "template skeletons" to determine if the person is in a benchmark zone</li>
            <li>To determine when the person is in a benchmark zone, compute the peaks of the similarity scores from the previous step</li>
            <li>Sample a window of skeletons centered at these peaks</li>
            <li>Compute the average similarity over the window with the template skeleton</li>
            <li>Compute the performance score to be a weighted sum of the similarities at each benchmark zone</li>
        </ol>
        Full flow diagram and modules are on the <a href="planning.html">planning</a> page.
        <br><br>
        <center><img src="images/static_diagram.png"/></center>
        <h4>Major Design Decisions</h4>
        <p><b>Skeleton Representation - </b>The problem with representing skeletons as the coordinates as their keypoints is that they are now sensitive to the scale and orientation of the subject. Our system represents skeletons as sets of relationships between points (i.e. distance, relative angle, ...). Looking at these relationships rather than the absolute points makes our system robust to changes in the composition of the shot.</p>
    </p> 
    <h3>Experiments and Qualitative Results</h3>
    <!--Provide details about the experimental set up (number of images/videos, number of datasets you experimented with, train/test split if you used machine learning algorithms, etc.). Describe the evaluation metrics you used to evaluate how well your approach is working. Include clear figures and tables, as well as illustrative qualitative examples if appropriate. Be sure to include obvious baselines to see if your approach is doing better than a naive approach (e.g. for classification accuracy, how well would a classifier do that made random decisions?). Also discuss any parameters of your algorithms, and tell us how you set the values of those parameters. You can also show us how the performance varies as you change those parameter values. Be sure to discuss any trends you see in your results, and explain why these trends make sense. Are the results as expected? Why?-->
    <!--Show several visual examples of inputs/outputs of your system (success cases and failures) that help us better understand your approach.-->
    <h3>Conclusion and Future Work</h3>
    <!-- Conclusion would likely make the same points as the abstract. Discuss any future ideas you have to make your approach better.-->
    
    <h3>References</h3>
    <p><a href="https://arxiv.org/abs/1611.08050">Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields</a> - Cao et al. 2017</p>
    <p><a href="https://arxiv.org/abs/1704.07809">Hand Keypoint Detection in Single Images using Multiview Bootstrapping</a> - Simon et al. 2017</p>
    <p><a href="https://arxiv.org/abs/1602.00134">Convolutional Pose Machines</a> - Wei et al. 2016</p>
    <p><a href="http://cocodataset.org/#home">COCO Object Detector</a> - Common Objects in Context</p>
    </body>
